{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.optimize as optim\n",
    "from helpers import *\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ZemelTransform(df_train):\n",
    "    \"\"\"\n",
    "    This function learns the model from the training set for Adult data which is binarized (in the steps below) and passed to the LBFGS solver\n",
    "    after computing the composite function using helpers.py. helpers.py has all the functions used in Zemel's paper as a s\n",
    "    separate module. As given in the paper the parameter setting (0.01,1,50) with\n",
    "    a k=10 gives good accuracy and reasonable discrimination for Adult dataset. Another setting of (0.01,0.01,0) gives low\n",
    "    discrimination. k value can be modified in the code below or passed as a parameter.\n",
    "    \"\"\"\n",
    "    k=5\n",
    "    encoded_data=df_train\n",
    "    catlabelsage=pd.get_dummies(encoded_data['age_cat'])\n",
    "    catlabelsdegree=pd.get_dummies(encoded_data['c_charge_degree'])\n",
    "    catlabelsprior=pd.get_dummies(encoded_data['priors_count'])\n",
    "    encoded_data=pd.concat([encoded_data,catlabelsage],axis=1)\n",
    "    encoded_data=encoded_data.drop(['age_cat'],axis=1)\n",
    "    encoded_data=pd.concat([encoded_data,catlabelsdegree],axis=1)\n",
    "    encoded_data=encoded_data.drop(['c_charge_degree'],axis=1)\n",
    "    encoded_data=pd.concat([encoded_data,catlabelsprior],axis=1)\n",
    "    encoded_data=encoded_data.drop(['priors_count'],axis=1)\n",
    "    encoders = {}\n",
    "    encoders['race'] = preprocessing.LabelEncoder()\n",
    "    encoded_data['race'] = encoders['race'].fit_transform(encoded_data['race'])\n",
    "    sensitive_idx = np.array(np.where(encoded_data['race']==0))[0].flatten()\n",
    "    nonsensitive_idx = np.array(np.where(encoded_data['race']!=0))[0].flatten()\n",
    "    original_sensitive=df_train.iloc[sensitive_idx,:]\n",
    "    original_nonsensitive=df_train.iloc[nonsensitive_idx,:]\n",
    "    originaldat=pd.concat((original_sensitive,original_nonsensitive))\n",
    "    ytrain_sensitive=np.array(encoded_data['is_recid'])[sensitive_idx]\n",
    "    ytrain_nonsensitive=np.array(encoded_data['is_recid'])[nonsensitive_idx]\n",
    "    encoded_data=encoded_data.drop('is_recid',axis=1)\n",
    "    traindat=np.array(encoded_data)\n",
    "    training_sensitive=traindat[sensitive_idx,:]\n",
    "    training_nonsensitive=traindat[nonsensitive_idx,:]\n",
    "    rez = np.random.uniform(size=traindat.shape[1] * 2 + k + traindat.shape[1] * k)\n",
    "    bnd = []\n",
    "    for i, k2 in enumerate(rez):\n",
    "        if i < traindat.shape[1] * 2 or i >= traindat.shape[1] * 2 + k:\n",
    "            bnd.append((None, None))\n",
    "        else:\n",
    "            bnd.append((0, 1))\n",
    "    rez = optim.fmin_l_bfgs_b(LFR, x0=rez, epsilon=1e-5, \n",
    "                          args=(training_sensitive, training_nonsensitive, \n",
    "                                ytrain_sensitive, ytrain_nonsensitive, k, 0.01,\n",
    "                                1, 50, 0),\n",
    "                          bounds = bnd, approx_grad=True, maxfun=15000, \n",
    "                          maxiter=15000)\n",
    "    \n",
    "    # extract values from obtained result vector\n",
    "    Ns, P = training_sensitive.shape\n",
    "    N,_=training_nonsensitive.shape\n",
    "    # alpha values for sensitive (0 after sklearn label encoder) and non-sensitive (1) groups.\n",
    "    alphaoptim0 = rez[0][:P]\n",
    "    alphaoptim1 = rez[0][P : 2 * P]\n",
    "    # weight vector\n",
    "    woptim = rez[0][2 * P : (2 * P) + k]\n",
    "    #cluster representatives\n",
    "    voptim = np.matrix(rez[0][(2 * P) + k:]).reshape((k, P))\n",
    "    \n",
    "    # learned distance function\n",
    "    dist_sensitive=distances(training_sensitive, voptim, alphaoptim1, Ns, P, k)\n",
    "    dist_nonsensitive=distances(training_nonsensitive,voptim,alphaoptim0,N,P,k)\n",
    "    \n",
    "    # learned cluster mapping probabilities\n",
    "    M_nk_sensitive=M_nk(dist_sensitive, Ns, k)\n",
    "    M_nk_nonsensitive=M_nk(dist_nonsensitive,N,k)\n",
    "    \n",
    "    # learned mappings\n",
    "    res_sensitive=x_n_hat(training_sensitive, M_nk_sensitive, voptim, Ns, P, k)\n",
    "    x_n_hat_sensitive=res_sensitive[0]\n",
    "    res_nonsensitive=x_n_hat(training_nonsensitive, M_nk_nonsensitive, voptim, N, P, k)\n",
    "    x_n_hat_nonsensitive=res_nonsensitive[0]\n",
    "    \n",
    "    # compute predictions for training dataset\n",
    "    res_sensitive=yhat(M_nk_sensitive, ytrain_sensitive, woptim, Ns, k)\n",
    "    y_hat_sensitive=res_sensitive[0]\n",
    "    res_nonsensitive=yhat(M_nk_nonsensitive, ytrain_nonsensitive, woptim, N, k)\n",
    "    y_hat_nonsensitive=res_nonsensitive[0]\n",
    "    \n",
    "    # preserve ordering (done here because of how the implementation is to minimize confusion for me!)\n",
    "    yordered=np.concatenate((ytrain_sensitive,ytrain_nonsensitive))\n",
    "    #ordered final train predictions\n",
    "    yhatordered=np.concatenate((y_hat_sensitive,y_hat_nonsensitive))\n",
    "    \n",
    "    traindataordered=np.concatenate((training_sensitive,training_nonsensitive))\n",
    "    # ordered final train mappings\n",
    "    traindatahatordered=np.concatenate((x_n_hat_sensitive,x_n_hat_nonsensitive))\n",
    "        \n",
    "    return originaldat,traindataordered,yordered,traindatahatordered,yhatordered,rez\n",
    "\n",
    "def ZemelPrediction(testing_sensitive,testing_nonsensitive,ytest_sensitive,ytest_nonsensitive,rez):\n",
    "    \"\"\"\n",
    "    This code uses the model learned from the training data and applies that model (rez) on the test data to obtain the final\n",
    "    yhat predictions (yhat=ytilde here as there is no separate prediction algorithm applied here). The resulting predictions\n",
    "    on test are returned.\n",
    "    \"\"\"\n",
    "    k=5\n",
    "    # extract training model parameters\n",
    "    Ns, P = testing_sensitive.shape\n",
    "    N,_=testing_nonsensitive.shape\n",
    "    alphaoptim0 = rez[0][:P]\n",
    "    alphaoptim1 = rez[0][P : 2 * P]\n",
    "    woptim = rez[0][2 * P : (2 * P) + k]\n",
    "    voptim = np.matrix(rez[0][(2 * P) + k:]).reshape((k, P))\n",
    "    \n",
    "    # compute distances on the test dataset using train model params\n",
    "    dist_sensitive=distances(testing_sensitive, voptim, alphaoptim1, Ns, P, k)\n",
    "    dist_nonsensitive=distances(testing_nonsensitive,voptim,alphaoptim0,N,P,k)\n",
    "    \n",
    "    #compute cluster probabilities for test instances\n",
    "    M_nk_sensitive=M_nk(dist_sensitive, Ns, k)\n",
    "    M_nk_nonsensitive=M_nk(dist_nonsensitive,N,k)\n",
    "    \n",
    "    #compute predictions for test instances\n",
    "    res_sensitive=yhat(M_nk_sensitive, ytest_sensitive, woptim, Ns, k)\n",
    "    y_hat_sensitive=res_sensitive[0]\n",
    "    res_nonsensitive=yhat(M_nk_nonsensitive, ytest_nonsensitive, woptim, N, k)\n",
    "    y_hat_nonsensitive=res_nonsensitive[0]\n",
    "    \n",
    "    # return ordered test response and test predictions \n",
    "    yordered=np.concatenate((ytest_sensitive,ytest_nonsensitive))\n",
    "    yhatordered=np.concatenate((y_hat_sensitive,y_hat_nonsensitive))\n",
    "    return yordered,yhatordered\n",
    "\n",
    "def ZemelTestPreprocessing(df_test_new):\n",
    "    \"\"\"\n",
    "    This code uses the model learned from the training data and applies that model (rez) on the test data to obtain the final\n",
    "    yhat predictions (yhat=ytilde here as there is no separate prediction algorithm applied here). The resulting predictions\n",
    "    on test are returned.\n",
    "    \"\"\"\n",
    "    encoded_data=df_test_new\n",
    "    catlabelsage=pd.get_dummies(encoded_data['age_cat'])\n",
    "    catlabelsdegree=pd.get_dummies(encoded_data['c_charge_degree'])\n",
    "    catlabelsprior=pd.get_dummies(encoded_data['priors_count'])\n",
    "    encoded_data=pd.concat([encoded_data,catlabelsage],axis=1)\n",
    "    encoded_data=encoded_data.drop(['age_cat'],axis=1)\n",
    "    encoded_data=pd.concat([encoded_data,catlabelsdegree],axis=1)\n",
    "    encoded_data=encoded_data.drop(['c_charge_degree'],axis=1)\n",
    "    encoded_data=pd.concat([encoded_data,catlabelsprior],axis=1)\n",
    "    encoded_data=encoded_data.drop(['priors_count'],axis=1)\n",
    "    encoders = {}\n",
    "    encoders['race'] = preprocessing.LabelEncoder()\n",
    "    encoded_data['race'] = encoders['race'].fit_transform(encoded_data['race'])\n",
    "    sensitive_idx = np.array(np.where(encoded_data['race']==0))[0].flatten()\n",
    "    nonsensitive_idx = np.array(np.where(encoded_data['race']!=0))[0].flatten()\n",
    "    original_sensitive=df_test_new.iloc[sensitive_idx,:]\n",
    "    original_nonsensitive=df_test_new.iloc[nonsensitive_idx,:]\n",
    "    originaldat=pd.concat((original_sensitive,original_nonsensitive))\n",
    "    ytest_sensitive=np.array(encoded_data['is_recid'])[sensitive_idx]\n",
    "    ytest_nonsensitive=np.array(encoded_data['is_recid'])[nonsensitive_idx]\n",
    "    encoded_data=encoded_data.drop('is_recid',axis=1)\n",
    "    testdat=np.array(encoded_data)\n",
    "    testing_sensitive=testdat[sensitive_idx,:]\n",
    "    testing_nonsensitive=testdat[nonsensitive_idx,:]\n",
    "    return testing_sensitive,testing_nonsensitive,ytest_sensitive,ytest_nonsensitive,originaldat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path =r'../experiment_data1/'\n",
    "#These are the test and train datasets by splitting the original data.\n",
    "train_0 = pd.read_csv(path + \"train_0.csv\",index_col=None, header=0, usecols=range(1,6))\n",
    "test_0 = pd.read_csv(path + \"test_0.csv\",index_col=None, header=0, usecols=range(1,6))\n",
    "train_1 = pd.read_csv(path + \"train_1.csv\",index_col=None, header=0, usecols=range(1,6))\n",
    "test_1 = pd.read_csv(path + \"test_1.csv\",index_col=None, header=0, usecols=range(1,6))\n",
    "train_2 = pd.read_csv(path + \"train_2.csv\",index_col=None, header=0, usecols=range(1,6))\n",
    "test_2 = pd.read_csv(path + \"test_2.csv\",index_col=None, header=0, usecols=range(1,6))\n",
    "train_3 = pd.read_csv(path + \"train_3.csv\",index_col=None, header=0, usecols=range(1,6))\n",
    "test_3 = pd.read_csv(path + \"test_3.csv\",index_col=None, header=0, usecols=range(1,6))\n",
    "train_4 = pd.read_csv(path + \"train_4.csv\",index_col=None, header=0, usecols=range(1,6))\n",
    "test_4 = pd.read_csv(path + \"test_4.csv\",index_col=None, header=0, usecols=range(1,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhanu/anaconda3/envs/python2/lib/python2.7/site-packages/numba/dataflow.py:346: RuntimeWarning: Python2 style print partially supported.  Please use Python3 style print.\n",
      "  \"Python3 style print.\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 2827.0264296800856)\n",
      "\n",
      "(500, 2730.799685339796)\n",
      "\n",
      "(750, 2703.768110052205)\n",
      "\n",
      "(1000, 2696.9427774723995)\n",
      "\n",
      "(1250, 2690.25833051247)\n",
      "\n",
      "(1500, 2687.738689087813)\n",
      "\n",
      "(1750, 2683.860267600478)\n",
      "\n",
      "(2000, 2680.940930083381)\n",
      "\n",
      "(2250, 2679.4818366584877)\n",
      "\n",
      "(2500, 2676.715933602368)\n",
      "\n",
      "(2750, 2673.1806336620466)\n",
      "\n",
      "(3000, 2670.397473064959)\n",
      "\n",
      "(3250, 2665.2146845807883)\n",
      "\n",
      "(3500, 2663.826000611942)\n",
      "\n",
      "(3750, 2662.1582290498386)\n",
      "\n",
      "(4000, 2660.2375858881214)\n",
      "\n",
      "(4250, 2658.8488681690596)\n",
      "\n",
      "(4500, 2657.987690459391)\n",
      "\n",
      "(4750, 2657.724880831128)\n",
      "\n",
      "(5000, 2657.5523178845174)\n",
      "\n",
      "(5250, 2657.301586622756)\n",
      "\n",
      "(5500, 2657.1398984200496)\n",
      "\n",
      "(5750, 2656.644183353676)\n",
      "\n",
      "(6000, 2656.6831656552936)\n",
      "\n",
      "(6250, 2656.1110692160623)\n",
      "\n",
      "(6500, 2655.8218755268754)\n",
      "\n",
      "(6750, 2655.5808375813253)\n",
      "\n",
      "(7000, 2655.432164205184)\n",
      "\n",
      "(7250, 2655.3668509712547)\n",
      "\n",
      "(7500, 2655.3757977797973)\n",
      "\n",
      "(7750, 2655.0522153171205)\n",
      "\n",
      "(8000, 2654.882783795941)\n",
      "\n",
      "(8250, 2654.5305859816513)\n",
      "\n",
      "(8500, 2654.3123081472336)\n",
      "\n",
      "(8750, 2654.21604120113)\n",
      "\n",
      "(9000, 2653.98047248502)\n",
      "\n",
      "(9250, 2653.254485578787)\n",
      "\n",
      "(9500, 2652.4425088688868)\n",
      "\n",
      "(9750, 2652.440248753336)\n",
      "\n",
      "(10000, 2651.843463014628)\n",
      "\n",
      "(10250, 2651.078254672375)\n",
      "\n",
      "(10500, 2650.657086420945)\n",
      "\n",
      "(10750, 2650.352032875984)\n",
      "\n",
      "(11000, 2649.9834023826425)\n",
      "\n",
      "(11250, 2649.2906186055084)\n",
      "\n",
      "(11500, 2648.7925636174027)\n",
      "\n",
      "(11750, 2648.5466788951294)\n",
      "\n",
      "(12000, 2686.4395964448177)\n",
      "\n",
      "(12250, 2648.15961374436)\n",
      "\n",
      "(12500, 2647.960884613115)\n",
      "\n",
      "(12750, 2647.77412344847)\n",
      "\n",
      "(13000, 2647.615323536321)\n",
      "\n",
      "(13250, 2647.3292458267742)\n",
      "\n",
      "(13500, 2647.441164029992)\n",
      "\n",
      "(13750, 2647.3257411356158)\n",
      "\n",
      "(14000, 2647.325893745792)\n",
      "\n",
      "(14250, 2647.3268941647866)\n",
      "\n",
      "(14500, 2647.2184902870267)\n",
      "\n",
      "(14750, 2647.217577023596)\n",
      "\n",
      "(15000, 2647.2095918848954)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zemel_df_train_res=ZemelTransform(train_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "Zemel algorithm (ICML 2013) performance:\n",
      "Train performance with pert. dataset: \n",
      "0.717971359552\n",
      "Perturbed test performance when scored on original test y variable: \n",
      "0.706106966066\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.33682689179233449"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from __future__ import division\n",
    "\n",
    "print '----------------------------------------------------------------'\n",
    "print 'Zemel algorithm (ICML 2013) performance:'\n",
    "response=zemel_df_train_res[2]\n",
    "pred=zemel_df_train_res[4]\n",
    "print 'Train performance with pert. dataset: '\n",
    "print roc_auc_score(response,pred)\n",
    "\n",
    "zemel_preprocess=ZemelTestPreprocessing(test_4)\n",
    "zemel_prediction= ZemelPrediction(zemel_preprocess[0],zemel_preprocess[1],zemel_preprocess[2],zemel_preprocess[3],zemel_df_train_res[5])\n",
    "print 'Perturbed test performance when scored on original test y variable: '\n",
    "print roc_auc_score(zemel_prediction[0],zemel_prediction[1])\n",
    "\n",
    "# save performance\n",
    "df_test_pred = zemel_preprocess[4]\n",
    "df_test_pred['pred'] = zemel_prediction[1]\n",
    "\n",
    "mean = df_test_pred.groupby('race')['pred'].mean()\n",
    "v = mean.values\n",
    "v = v.reshape(len(v),1)\n",
    "ratio_df = pd.DataFrame(v/v.transpose(),index=mean.index,columns=mean.index )\n",
    "ratio_df_arr=np.asarray(np.abs(1-ratio_df))\n",
    "zemel_discrim=np.amax(ratio_df_arr)\n",
    "zemel_discrim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
